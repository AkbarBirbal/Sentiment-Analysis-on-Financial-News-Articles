{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from random import shuffle\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        flipped = {}\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]) \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append((utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabItem:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, fi, min_count):\n",
    "        vocab_items = []\n",
    "        vocab_hash = {}\n",
    "        word_count = 0\n",
    "        #fi = open(fi, 'r')\n",
    "        # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "        for token in ['<bol>', '<eol>']:\n",
    "            vocab_hash[token] = len(vocab_items)\n",
    "            vocab_items.append(VocabItem(token))\n",
    "        for line in fi:\n",
    "            tokens = line[0]\n",
    "            #print(\"\\rReading line %s\" %tokens)\n",
    "            for token in tokens:\n",
    "                if token not in vocab_hash:\n",
    "                    vocab_hash[token] = len(vocab_items)\n",
    "                    #print (\"\\r\\r token %s\" %token)\n",
    "                    #print (\"\\t\\t token value\",vocab_hash[token])\n",
    "                    vocab_items.append(VocabItem(token))\n",
    "                #assert vocab_items[vocab_hash[token]].word == token, 'Wrong vocab_hash index'\n",
    "                vocab_items[vocab_hash[token]].count += 1\n",
    "                word_count += 1\n",
    "                if word_count % 10000 == 0:\n",
    "                    sys.stdout.write(\"\\rReading word %d\" % word_count)\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "            vocab_items[vocab_hash['<bol>']].count += 1\n",
    "            vocab_items[vocab_hash['<eol>']].count += 1\n",
    "            word_count += 2\n",
    "        self.vocab_items = vocab_items # List of VocabItem objects\n",
    "        self.vocab_hash = vocab_hash  # Mapping from each token to its index in vocab\n",
    "        self.word_count = word_count # Total number of words in train file\n",
    "        # Add special token <unk> (unknown),\n",
    "        # merge words occurring less than min_count into <unk>, and\n",
    "        # sort vocab in descending order by frequency in train file\n",
    "        self.__sort(min_count)\n",
    "        print ('Total words in training file: %d' % self.word_count)\n",
    "        #print ('Total bytes in training file: %d' % self.bytes)\n",
    "        print ('Vocab size: %d' % len(self))\n",
    "    def __getitem__(self, i):\n",
    "        return self.vocab_items[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_items)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vocab_items)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.vocab_hash\n",
    "\n",
    "    def __sort(self, min_count):\n",
    "        tmp = []\n",
    "        tmp.append(VocabItem('<unk>'))\n",
    "        unk_hash = 0\n",
    "        \n",
    "        count_unk = 0\n",
    "        for token in self.vocab_items:\n",
    "            if token.count < min_count:\n",
    "                count_unk += 1\n",
    "                tmp[unk_hash].count += token.count\n",
    "                #print(\"word setting as unknow:\",token.word)\n",
    "            else:\n",
    "                tmp.append(token)\n",
    "\n",
    "        tmp.sort(key=lambda token : token.count, reverse=True)\n",
    "\n",
    "        # Update vocab_hash\n",
    "        vocab_hash = {}\n",
    "        for i, token in enumerate(tmp):\n",
    "            vocab_hash[token.word] = i\n",
    "\n",
    "        self.vocab_items = tmp\n",
    "        self.vocab_hash = vocab_hash\n",
    "        #print (\"printing vocab_hash\")\n",
    "        #for key,value in vocab_hash.items():\n",
    "         #   print (key,value)\n",
    "        #print ('Unknown vocab size:', count_unk)\n",
    "\n",
    "    def indices(self, tokens):\n",
    "        return [self.vocab_hash[token] if token in self else self.vocab_hash['<unk>'] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class paraItem:\n",
    "    def __init__(self,par):\n",
    "        self.label = par\n",
    "        self.wc = 0\n",
    "        self.filename = None\n",
    "        self.dmvec = np.random.uniform(low=-0.5/100, high=0.5/100, size=(100)) #for every phrase size of dim\n",
    "        self.words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class paragrahps:\n",
    "    def __init__(self,fi):\n",
    "        paras =[]\n",
    "        for line in fi:\n",
    "            tokens = line[0]\n",
    "            if tokens:\n",
    "                paras.append(paraItem(line[1][0])) #list of objects with sentence index as lable.\n",
    "                paras[len(paras)-1].words = tokens #add the words into objects. \n",
    "                paras[len(paras)-1].wc = len(tokens) # upated word count in the para\n",
    "            else:\n",
    "                pass\n",
    "        self.paras =paras\n",
    "    def __getlist__(self):\n",
    "        return self.paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'C:\\\\Users\\\\bpotinen\\\\financial'\n",
    "os.chdir( path )\n",
    "sources = {'neg.txt':'NEG', 'pos.txt':'POS','unk.txt':'UNK'}\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = sentences.to_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word 7160000Total words in training file: 7163532\n",
      "Vocab size: 55314\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(sent,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "paras = paragrahps(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "para_list = paras.__getlist__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import itertools \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UnigramTable:\n",
    "    \"\"\"\n",
    "    A list of indices of tokens in the vocab following a power law distribution,\n",
    "    used to draw negative samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        vocab_size = len(vocab)\n",
    "        power = 0.75\n",
    "        norm = sum([math.pow(t.count, power) for t in vocab]) # Normalizing constant\n",
    "        print (norm)\n",
    "        table_size = 1e8 # Length of the unigram table depends on vocab\n",
    "        #print table_size\n",
    "        table = np.zeros(table_size, dtype=np.uint32)\n",
    "\n",
    "        print ('Filling unigram table')\n",
    "        p = 0 # Cumulative probability\n",
    "        i = 0\n",
    "        old_i = 0 \n",
    "        for j, unigram in enumerate(vocab):\n",
    "            #print \"j\",j\n",
    "            #print \"unigram\",unigram\n",
    "            \n",
    "            p += float(math.pow(unigram.count, power))/norm\n",
    "            while i < table_size and float(i) / table_size < p:\n",
    "                table[i] = j\n",
    "                i += 1\n",
    "            old_i = i - old_i\n",
    "            sys.stdout.write(\"\\r propability for word '%s' is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "            sys.stdout.flush()\n",
    "            #print(\"propability for word %s is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "        self.table = table\n",
    "    def sample(self, count):\n",
    "        indices = np.random.randint(low=0, high=len(self.table), size=count)\n",
    "        return [self.table[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1197825.719168978\n",
      "Filling unigram table\n",
      " propability for word 'Bhaumik' is 1.000000, kept it  49729009 times"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bpotinen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "table = UnigramTable(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(dim, vocab_size):\n",
    "    # Init input words with random numbers from a uniform distribution on the interval [-0.5, 0.5]/dim\n",
    "    tmp = np.random.uniform(low=-0.5/dim, high=0.5/dim, size=(vocab_size, dim))\n",
    "    input_word = tmp \n",
    "    # Init weights with zeros\n",
    "    tmp = np.zeros(shape=(vocab_size, dim))\n",
    "    weights = tmp\n",
    "  \n",
    "\n",
    "    return (input_word,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): #sigmoid function goes from -6 to +6\n",
    "    if z > 6:\n",
    "        return 1.0\n",
    "    elif z < -6:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_word,weights = initialize(100,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.000003  para Progress: 22968 of 22999 (99.87%)time taken 2960.363451929243\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "starting_alpha=0.025\n",
    "current_word_count=0  \n",
    "alpha_count = 0\n",
    "last_alpha_count = 0\n",
    "win=5 \n",
    "para_count = 0\n",
    "for i in para_list:\n",
    "    current_p = i\n",
    "    para_count+=1\n",
    "    sentence = current_p.words\n",
    "    ind = vocab.indices(sentence)\n",
    "    for sent_pos, token in enumerate(ind):\n",
    "        neu1e = np.zeros(100)\n",
    "        if current_word_count % 10000 == 0:\n",
    "            alpha_count += (current_word_count - last_alpha_count)\n",
    "            last_alpha_count = current_word_count\n",
    "            alpha = starting_alpha * (1 - float(alpha_count) / 17020851)\n",
    "            if alpha < starting_alpha * 0.0001: alpha = starting_alpha * 0.0001\n",
    "            sys.stdout.write(\"\\rAlpha: %f  para Progress: %d of %d (%.2f%%)\" %\n",
    "                                 (alpha, para_count, len(para_list),\n",
    "                                  float(para_count* 100/len(para_list))))\n",
    "            sys.stdout.flush()\n",
    "        current_win = np.random.randint(low=1, high=win+1)\n",
    "        context_start = max(sent_pos - current_win, 0)\n",
    "        context_end = min(sent_pos + current_win + 1, len(ind))\n",
    "        context = ind[context_start:sent_pos] + ind[sent_pos+1:context_end]\n",
    "        for context_word in context:\n",
    "            neu1e = np.zeros(100)\n",
    "            classifiers = [(token, 1)] + [(target, 0) for target in table.sample(10)]\n",
    "            for target, label in classifiers:\n",
    "                z = np.dot(input_word[context_word],weights[target])\n",
    "                p = sigmoid(z)\n",
    "                g = alpha * (label - p)\n",
    "                neu1e+=g*weights[target]\n",
    "                weights[target] +=g* input_word[context_word]\n",
    "            input_word[context_word] +=neu1e #updated word vector\n",
    "            current_p.dmvec+=neu1e #update para vector\n",
    "            current_word_count += 1    \n",
    "print ('time taken',time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.28410196e-05,   6.29690194e-04,  -1.11398223e-02,\n",
       "         5.08897221e-03,   1.39724287e-02,  -1.00116045e-02,\n",
       "        -1.28937798e-02,  -6.90131240e-03,  -5.96449961e-04,\n",
       "        -2.94534286e-02,  -1.67500308e-02,  -1.65756057e-02,\n",
       "        -2.26600929e-03,  -1.16309271e-02,   1.84101231e-02,\n",
       "         1.94836671e-02,   2.03266053e-04,   2.02008043e-02,\n",
       "        -1.67962719e-02,   1.77595092e-04,  -1.52958769e-02,\n",
       "        -1.27740226e-02,   1.63103281e-03,   2.21804442e-02,\n",
       "         1.39536675e-02,   1.39552261e-02,  -2.00838001e-03,\n",
       "        -1.59738557e-02,  -1.89997548e-02,  -1.20090873e-02,\n",
       "        -1.85338839e-02,  -1.71101703e-02,   1.12887440e-02,\n",
       "         2.99290874e-03,  -1.08594776e-02,  -1.04301611e-02,\n",
       "        -2.18665869e-02,  -6.62380178e-03,  -2.79377296e-03,\n",
       "        -2.81583471e-03,   9.25607146e-04,   7.87585078e-03,\n",
       "         2.28138325e-03,   7.28638681e-03,  -4.78388836e-03,\n",
       "        -1.17518613e-02,  -8.51945461e-03,   8.83125597e-03,\n",
       "         2.76297494e-03,  -7.03474735e-03,   1.10650596e-02,\n",
       "        -1.05819678e-04,  -5.78372688e-03,  -3.66971438e-03,\n",
       "        -4.30259614e-03,  -8.32954272e-03,  -9.78912340e-03,\n",
       "         2.15488523e-02,   7.19643617e-03,  -1.36911240e-02,\n",
       "         7.84950126e-03,  -2.12698883e-02,   3.34680933e-03,\n",
       "         1.71124712e-02,  -1.92088564e-02,   1.58898256e-02,\n",
       "         1.20494471e-02,   3.66022436e-02,  -2.07485623e-03,\n",
       "        -6.96209296e-03,  -1.27993372e-02,  -5.33771889e-04,\n",
       "        -3.54348895e-04,   1.10225817e-03,   1.63526133e-02,\n",
       "         9.04058091e-03,  -7.84220451e-04,  -5.77989305e-03,\n",
       "        -8.21079659e-03,   8.40842701e-03,   7.58667465e-04,\n",
       "         2.49430508e-02,  -2.82422941e-03,  -1.16511702e-02,\n",
       "         2.33905960e-03,  -2.46699896e-04,  -1.14901689e-02,\n",
       "        -5.82077343e-03,   1.73413625e-06,   2.73491773e-02,\n",
       "         3.35683702e-03,   2.88720972e-04,  -1.01087103e-02,\n",
       "        -7.91376482e-03,  -1.23382745e-02,  -7.29828734e-04,\n",
       "         1.43330112e-02,   2.23675290e-03,   1.05540017e-02,\n",
       "        -5.07687157e-04])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fo = open('financial_model', 'w',encoding='utf8')\n",
    "fo.write('%d %d\\n' % (len(para_list)+len(vocab), 100))\n",
    "for i in para_list:\n",
    "    label = i.label\n",
    "    vector_str = ' '.join([str(s) for s in i.dmvec])\n",
    "    fo.write('%s %s\\n' % (label, vector_str))\n",
    "for token, vector in zip(vocab, input_word):\n",
    "    word = token.word\n",
    "    vector_str = ' '.join([str(s) for s in vector])\n",
    "    fo.write('%s %s\\n' % (word, vector_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " model = Doc2Vec.load_word2vec_format('imdb_para2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NEG_146', 0.9941427111625671),\n",
       " ('NEG_183', 0.9939985275268555),\n",
       " ('NEG_140', 0.9939755201339722),\n",
       " ('NEG_148', 0.9939613342285156),\n",
       " ('NEG_71', 0.9939123392105103),\n",
       " ('NEG_110', 0.9938923120498657),\n",
       " ('Singardo', 0.9938366413116455),\n",
       " ('NEG_54', 0.9938062429428101),\n",
       " ('NEG_100', 0.9937904477119446),\n",
       " ('NEG_133', 0.9937737584114075)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('NEG_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data =[]\n",
    "for i in para_list:\n",
    "    if 'POS' in i.label:\n",
    "        pos_data.append((i.dmvec,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_data =[]\n",
    "for i in para_list:\n",
    "    if 'NEG' in i.label:\n",
    "        neg_data.append((i.dmvec,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2331"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13656"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data= pos_data[:2000]+neg_data[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for i in data:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88375\n"
     ]
    }
   ],
   "source": [
    "print (metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      1.00      0.90       400\n",
      "          1       1.00      0.77      0.87       400\n",
      "\n",
      "avg / total       0.91      0.88      0.88       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "preds= model2.predict_proba(X_test)[:,1]\n",
    "fpr,tpr,_ = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51  ,  0.5175,  0.5275,  0.8275,  0.8275,  0.83  ,  0.83  ,\n",
       "        0.8325,  0.8325,  0.835 ,  0.835 ,  0.8375,  0.8375,  0.84  ,\n",
       "        0.84  ,  0.8475,  0.8475,  0.85  ,  0.85  ,  0.855 ,  0.855 ,\n",
       "        0.8575,  0.8575,  0.86  ,  0.86  ,  0.8625,  0.8625,  0.865 ,\n",
       "        0.865 ,  0.87  ,  0.87  ,  0.8725,  0.8725,  0.875 ,  0.875 ,\n",
       "        0.8775,  0.8775,  0.88  ,  0.88  ,  0.8825,  0.8825,  0.885 ,\n",
       "        0.885 ,  0.8875,  0.8875,  0.89  ,  0.89  ,  0.8925,  0.8925,\n",
       "        0.895 ,  0.895 ,  0.8975,  0.8975,  0.9   ,  0.9   ,  0.9025,\n",
       "        0.9025,  0.905 ,  0.905 ,  0.91  ,  0.91  ,  0.9125,  0.9125,\n",
       "        0.915 ,  0.915 ,  0.9175,  0.9175,  0.92  ,  0.92  ,  0.9225,\n",
       "        0.9225,  0.925 ,  0.925 ,  0.9275,  0.9275,  0.93  ,  0.93  ,\n",
       "        0.9325,  0.9325,  0.935 ,  0.935 ,  0.9375,  0.9375,  0.94  ,\n",
       "        0.94  ,  0.9425,  0.9425,  0.945 ,  0.945 ,  0.9475,  0.9475,\n",
       "        0.95  ,  0.95  ,  0.9525,  0.9525,  0.955 ,  0.955 ,  0.9575,\n",
       "        0.9575,  0.96  ,  0.96  ,  0.9625,  0.9625,  0.965 ,  0.965 ,\n",
       "        0.9675,  0.9675,  0.97  ,  0.97  ,  0.9725,  0.9725,  0.975 ,\n",
       "        0.975 ,  0.9775,  0.9775,  0.98  ,  0.98  ,  0.9825,  0.9825,\n",
       "        0.9875,  0.9875,  0.99  ,  0.99  ,  0.9925,  0.9925,  0.995 ,\n",
       "        0.995 ,  0.9975,  0.9975,  1.    ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88375000000000004"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in para_list:\n",
    "    if 'NEG_10' in i.label:\n",
    "        words = i.words\n",
    "        vec=i.dmvec\n",
    "        vec = vec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stock market seems to recoil each time a bigticket acquisition is announced Experts at moneycontrol spoke to believe investors are worried about the financial risks of such costly acquisitions\n"
     ]
    }
   ],
   "source": [
    "print (' '.join(word for word in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7795666,  0.2204334]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict_proba(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ve = neg_data[33][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bpotinen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
