{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import itertools \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VocabItem:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, fi, min_count):\n",
    "        vocab_items = []\n",
    "        vocab_hash = {}\n",
    "        word_count = 0\n",
    "        fi = open(fi, 'r')\n",
    "        # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "        for token in ['<bol>', '<eol>']:\n",
    "            vocab_hash[token] = len(vocab_items)\n",
    "            vocab_items.append(VocabItem(token))\n",
    "        for line in fi:\n",
    "            tokens = line.split()\n",
    "            #print(\"\\rReading line %s\" %tokens)\n",
    "            for token in tokens:\n",
    "                if token not in vocab_hash:\n",
    "                    vocab_hash[token] = len(vocab_items)\n",
    "                    #print (\"\\r\\r token %s\" %token)\n",
    "                    #print (\"\\t\\t token value\",vocab_hash[token])\n",
    "                    vocab_items.append(VocabItem(token))\n",
    "                #assert vocab_items[vocab_hash[token]].word == token, 'Wrong vocab_hash index'\n",
    "                vocab_items[vocab_hash[token]].count += 1\n",
    "                word_count += 1\n",
    "                if word_count % 10000 == 0:\n",
    "                    sys.stdout.write(\"\\rReading word %d\" % word_count)\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            # Add special tokens <bol> (beginning of line) and <eol> (end of line)\n",
    "            vocab_items[vocab_hash['<bol>']].count += 1\n",
    "            vocab_items[vocab_hash['<eol>']].count += 1\n",
    "            word_count += 2\n",
    "        self.vocab_items = vocab_items # List of VocabItem objects\n",
    "        self.vocab_hash = vocab_hash  # Mapping from each token to its index in vocab\n",
    "        self.word_count = word_count # Total number of words in train file\n",
    "        # Add special token <unk> (unknown),\n",
    "        # merge words occurring less than min_count into <unk>, and\n",
    "        # sort vocab in descending order by frequency in train file\n",
    "        self.__sort(min_count)\n",
    "        print ('Total words in training file: %d' % self.word_count)\n",
    "        #print ('Total bytes in training file: %d' % self.bytes)\n",
    "        print ('Vocab size: %d' % len(self))\n",
    "    def __getitem__(self, i):\n",
    "        return self.vocab_items[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab_items)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vocab_items)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.vocab_hash\n",
    "\n",
    "    def __sort(self, min_count):\n",
    "        tmp = []\n",
    "        tmp.append(VocabItem('<unk>'))\n",
    "        unk_hash = 0\n",
    "        \n",
    "        count_unk = 0\n",
    "        for token in self.vocab_items:\n",
    "            if token.count < min_count:\n",
    "                count_unk += 1\n",
    "                tmp[unk_hash].count += token.count\n",
    "                #print(\"word setting as unknow:\",token.word)\n",
    "            else:\n",
    "                tmp.append(token)\n",
    "\n",
    "        tmp.sort(key=lambda token : token.count, reverse=True)\n",
    "\n",
    "        # Update vocab_hash\n",
    "        vocab_hash = {}\n",
    "        for i, token in enumerate(tmp):\n",
    "            vocab_hash[token.word] = i\n",
    "\n",
    "        self.vocab_items = tmp\n",
    "        self.vocab_hash = vocab_hash\n",
    "        #print (\"printing vocab_hash\")\n",
    "        #for key,value in vocab_hash.items():\n",
    "         #   print (key,value)\n",
    "        #print ('Unknown vocab size:', count_unk)\n",
    "\n",
    "    def indices(self, tokens):\n",
    "        return [self.vocab_hash[token] if token in self else self.vocab_hash['<unk>'] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class paraItem:\n",
    "    def __init__(self,par):\n",
    "        self.label = par\n",
    "        self.wc = 0\n",
    "        self.filename = None\n",
    "        self.dmvec = np.random.uniform(low=-0.5/100, high=0.5/100, size=(100)) #for every phrase size of dim\n",
    "        self.words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class paragrahps:\n",
    "    def __init__(self,fi,max_sentence_length):\n",
    "        paras =[]\n",
    "        local_w = 0\n",
    "        sen_count = 0 \n",
    "        curr_sen =[]\n",
    "        total_wc = 0\n",
    "        fi = open(fi,'r')\n",
    "        for line in fi:\n",
    "            tokens = line.split()\n",
    "            print (len(tokens))\n",
    "            for token in tokens:\n",
    "                if local_w <= max_sentence_length:\n",
    "                    curr_sen.append(token)\n",
    "                    local_w +=1 \n",
    "                    total_wc+=1\n",
    "                    #print (\"in if loop\")\n",
    "                else:\n",
    "                    local_w = 0 #reset the word count for sentence\n",
    "                    sys.stdout.write(\"\\rRead word %d and updating %d para object\" % (total_wc,sen_count))\n",
    "                    sys.stdout.flush()\n",
    "                    paras.append(paraItem(sen_count)) #list of objects with sentence index as lable.\n",
    "                    paras[len(paras)-1].words = curr_sen #add the words into objects. \n",
    "                    paras[len(paras)-1].wc = len(curr_sen) # upated word count in the para\n",
    "                    sen_count+=1 #increase the sentence count==> index to paraItems\n",
    "                    curr_sen=[] #clear the words to add new words.\n",
    "        self.paras =paras\n",
    "    def __getlist__(self):\n",
    "        return self.paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramTable:\n",
    "    \"\"\"\n",
    "    A list of indices of tokens in the vocab following a power law distribution,\n",
    "    used to draw negative samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        vocab_size = len(vocab)\n",
    "        power = 0.75\n",
    "        norm = sum([math.pow(t.count, power) for t in vocab]) # Normalizing constant\n",
    "        print (norm)\n",
    "        table_size = 1e8 # Length of the unigram table depends on vocab\n",
    "        #print table_size\n",
    "        table = np.zeros(table_size, dtype=np.uint32)\n",
    "\n",
    "        print ('Filling unigram table')\n",
    "        p = 0 # Cumulative probability\n",
    "        i = 0\n",
    "        old_i = 0 \n",
    "        for j, unigram in enumerate(vocab):\n",
    "            #print \"j\",j\n",
    "            #print \"unigram\",unigram\n",
    "            \n",
    "            p += float(math.pow(unigram.count, power))/norm\n",
    "            while i < table_size and float(i) / table_size < p:\n",
    "                table[i] = j\n",
    "                i += 1\n",
    "            old_i = i - old_i\n",
    "            sys.stdout.write(\"\\r propability for word '%s' is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "            sys.stdout.flush()\n",
    "            #print(\"propability for word %s is %f, kept it  %d times\" %(unigram.word,p,old_i))\n",
    "        self.table = table\n",
    "    def sample(self, count):\n",
    "        indices = np.random.randint(low=0, high=len(self.table), size=count)\n",
    "        return [self.table[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word 17000000Total words in training file: 17005209\n",
      "Vocab size: 71291\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary(\"text8\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      "Read word 17000850 and updating 849 para object"
     ]
    }
   ],
   "source": [
    "para_list = paragrahps(\"text8\",20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa = para_list.__getlist__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253145.7750768745\n",
      "Filling unigram table\n",
      " propability for word 'ngurunderi' is 0.999927, kept it  49587746 times"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bpotinen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:13: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "table = UnigramTable(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize(dim, vocab_size):\n",
    "    # Init input words with random numbers from a uniform distribution on the interval [-0.5, 0.5]/dim\n",
    "    tmp = np.random.uniform(low=-0.5/dim, high=0.5/dim, size=(vocab_size, dim))\n",
    "    input_word = tmp \n",
    "    # Init weights with zeros\n",
    "    tmp = np.zeros(shape=(vocab_size, dim))\n",
    "    weights = tmp\n",
    "  \n",
    "\n",
    "    return (input_word,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): #sigmoid function goes from -6 to +6\n",
    "    if z > 6:\n",
    "        return 1.0\n",
    "    elif z < -6:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1 / (1 + math.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_word,weights = initialize(100,len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.000003  para Progress: 504 of 850 (59.29%)"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "starting_alpha=0.025\n",
    "current_word_count=0  \n",
    "alpha_count = 0\n",
    "last_alpha_count = 0\n",
    "win=5 \n",
    "for i in pa:\n",
    "    current_p = i\n",
    "    sentence = current_p.words\n",
    "    ind = vocab.indices(sentence)\n",
    "    for sent_pos, token in enumerate(ind):\n",
    "        neu1e = np.zeros(100)\n",
    "        if current_word_count % 10000 == 0:\n",
    "            alpha_count += (current_word_count - last_alpha_count)\n",
    "            last_alpha_count = current_word_count\n",
    "            alpha = starting_alpha * (1 - float(alpha_count) / 17020851)\n",
    "            if alpha < starting_alpha * 0.0001: alpha = starting_alpha * 0.0001\n",
    "            sys.stdout.write(\"\\rAlpha: %f  para Progress: %d of %d (%.2f%%)\" %\n",
    "                                 (alpha, current_p.label, len(pa),\n",
    "                                  float(current_p.label* 100/len(pa))))\n",
    "            sys.stdout.flush()\n",
    "        current_win = np.random.randint(low=1, high=win+1)\n",
    "        context_start = max(sent_pos - current_win, 0)\n",
    "        context_end = min(sent_pos + current_win + 1, len(ind))\n",
    "        context = ind[context_start:sent_pos] + ind[sent_pos+1:context_end]\n",
    "        for context_word in context:\n",
    "            neu1e = np.zeros(100)\n",
    "            classifiers = [(token, 1)] + [(target, 0) for target in table.sample(10)]\n",
    "            for target, label in classifiers:\n",
    "                z = np.dot(input_word[context_word],weights[target])\n",
    "                p = sigmoid(z)\n",
    "                g = alpha * (label - p)\n",
    "                neu1e+=g*weights[target]\n",
    "                weights[target] +=g* input_word[context_word]\n",
    "            input_word[context_word] +=neu1e\n",
    "            current_p.dmvec+=neu1e\n",
    "            current_word_count += 1    \n",
    "print ('time taken',time.clock() - start)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
